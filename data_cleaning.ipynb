{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7e03609-cc80-41d8-ae6f-3d5f83ac4ffc",
   "metadata": {},
   "source": [
    "#  Nettoyage et Pr√©paration des Donn√©es - Projet Pr√©diction de Salaire\r\n",
    "\r\n",
    "Ce notebook a pour objectif de nettoyer et pr√©parer les donn√©es issues du scraping des offres d‚Äôemploi (HelloWork, Indeed, etc.) \r\n",
    "afin d‚Äôobtenir un jeu de donn√©es propre, coh√©rent et exploitable pour la phase de Machine Learning.\r\n",
    "\r\n",
    "### Objectifs principaux :\r\n",
    "- Fusionner et centraliser les fichiers CSV\r\n",
    "- Nettoyer les textes (titres, descriptions)\r\n",
    "- Uniformiser les salaires (‚Ç¨/an)\r\n",
    "- G√©rer les valeurs manquantes\r\n",
    "- Sauvegarder un dataset final propre : `cleaned_jobs.csv`\r\n",
    "\r\n",
    "### Donn√©es utilis√©es :\r\n",
    "- Donn√©es issues du scraping (`data_ha_indeed.csv`)\r\n",
    "- Champs principaux : `title`, `company`, `location`, `salary`, `description`\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40420cf9-e06a-488e-9889-36c961b594ab",
   "metadata": {},
   "source": [
    "##  Importation des librairies n√©cessaires\r\n",
    "Nous utilisons `pandas` pour la manipulation des donn√©es, `re` pour le nettoyage des textes, \r\n",
    "et `matplotlib` pour visualiser certaines distribution  \n",
    "#üìÇ Chargement et aper√ßu des donn√©es\r\n",
    "Nous chargeons les fichiers CSV issus du scraping et v√©rifions leur structure\n",
    "\n",
    "s.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3e5c9c-a274-4c97-8eb4-1e5e2471969a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "\n",
    "\n",
    "df1 = pd.read_csv(\"C:/Users/Etu/Desktop/offres_developpeur_mobile.csv\" ,  encoding = \"utf-8\") \n",
    "df2 = pd.read_csv(\"C:/Users/Etu/Desktop/filtered_developpeur_logicielll.csv\" ,  encoding = \"utf-8\") \n",
    "df3 = pd.read_csv(\"C:/Users/Etu/Desktop/filtered_developpeur_webb.csv\" ,  encoding = \"utf-8\")\n",
    "df4 = pd.read_csv(\"C:/Users/Etu/Desktop/data_analystt.csv\" ,  encoding = \"utf-8\")\n",
    "df5 = pd.read_csv(\"C:/Users/Etu/Desktop/data engineer competencee.csv\" ,  encoding = \"utf-8\")\n",
    "df6 = pd.read_csv(\"C:/Users/Etu/Desktop/data scientist competence.csv\" ,  encoding = \"utf-8\")\n",
    "df7 = pd.read_csv(\"C:/Users/Etu/Desktop/filtered informatique competencee.csv\" ,  encoding = \"utf-8\")\n",
    "\n",
    "\n",
    "for df in [df1, df2,df3,df4,df5,df6,df7]:\n",
    "    df.columns = df.columns.str.strip().str.lower()  \n",
    "\n",
    "# Merge both datasets\n",
    "merged_df = pd.concat([df1, df2,df3,df4,df5,df6,df7], ignore_index=True)  \n",
    "\n",
    "\n",
    "\n",
    "# Save to a new CSV file\n",
    "merged_df.to_csv(\"C:/Users/Etu/Desktop/domaine_info.csv\", index=False ,  encoding = \"utf-8-sig\")  \n",
    "\n",
    "len(merged_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cc8b79-b3f0-4852-b6fd-e631a39296f4",
   "metadata": {},
   "source": [
    "On observe que certaines colonnes comme `salary` ou `experience` contiennent des valeurs manquantes ou mal format√©es.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c10a0dbd-199a-4ac3-a38f-d5e7f4c84783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3067 entries, 0 to 3066\n",
      "Data columns (total 14 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   id_offre            3067 non-null   object \n",
      " 1   titre               3067 non-null   object \n",
      " 2   entreprise          2320 non-null   object \n",
      " 3   salaire             3067 non-null   object \n",
      " 4   salaire_complement  488 non-null    object \n",
      " 5   ville               3067 non-null   object \n",
      " 6   region              0 non-null      float64\n",
      " 7   description         3067 non-null   object \n",
      " 8   date_publication    3067 non-null   object \n",
      " 9   experience          3067 non-null   object \n",
      " 10  contrat             3067 non-null   object \n",
      " 11  competences         767 non-null    object \n",
      " 12  departement         3067 non-null   object \n",
      " 13  mot_cle_source      3067 non-null   object \n",
      "dtypes: float64(1), object(13)\n",
      "memory usage: 335.6+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_offre</th>\n",
       "      <th>titre</th>\n",
       "      <th>entreprise</th>\n",
       "      <th>salaire</th>\n",
       "      <th>salaire_complement</th>\n",
       "      <th>ville</th>\n",
       "      <th>region</th>\n",
       "      <th>description</th>\n",
       "      <th>date_publication</th>\n",
       "      <th>experience</th>\n",
       "      <th>contrat</th>\n",
       "      <th>competences</th>\n",
       "      <th>departement</th>\n",
       "      <th>mot_cle_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>201STWH</td>\n",
       "      <td>Ing√©nieur.e d'√©tude : traitement de donn√©es/si...</td>\n",
       "      <td>INRAE CLERMONT-ARA ST-GENES-CHAMPANELLE</td>\n",
       "      <td>Mensuel de 2244.79 Euros sur 12.0 mois</td>\n",
       "      <td>NaN</td>\n",
       "      <td>03 - Montoldre</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Vous exercerez votre activit√© au sein de l'Uni...</td>\n",
       "      <td>2025-12-19T10:23:24.260Z</td>\n",
       "      <td>D√©butant accept√©</td>\n",
       "      <td>CDD</td>\n",
       "      <td>Analyser, exploiter, structurer des donn√©es</td>\n",
       "      <td>3</td>\n",
       "      <td>data engineer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>201CSRN</td>\n",
       "      <td>Data Engineer (H/F)</td>\n",
       "      <td>SOCIETE POUR L'INFORMATIQUE INDUSTRIELLE</td>\n",
       "      <td>Mensuel de 37000.0 Euros √† 40000.0 Euros sur 1...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>06 - VALBONNE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Dans le cadre du d√©veloppement de notre agence...</td>\n",
       "      <td>2025-12-03T16:20:41.796Z</td>\n",
       "      <td>D√©butant accept√©</td>\n",
       "      <td>CDI</td>\n",
       "      <td>Analyser, exploiter, structurer des donn√©es</td>\n",
       "      <td>6</td>\n",
       "      <td>data engineer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6683253</td>\n",
       "      <td>Charg√© de mission et projets (H/F)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Annuel de 40000.0 Euros √† 50000.0 Euros</td>\n",
       "      <td>NaN</td>\n",
       "      <td>06 - Valbonne</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Offre d&amp;apos,emploi : CDI Client final Chef de...</td>\n",
       "      <td>2025-12-17T09:02:07.000Z</td>\n",
       "      <td>D√©butant accept√©</td>\n",
       "      <td>CDI</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>consultant business intelligence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3675058</td>\n",
       "      <td>DATA ANALYST ‚Äì TEMPS PLEIN H/F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Annuel de 0.0 Euros</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10 - Nogent-sur-Seine</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Votre mission :   Pr√©parer un plan de travail ...</td>\n",
       "      <td>2025-10-11T04:21:38.000Z</td>\n",
       "      <td>D√©butant accept√©</td>\n",
       "      <td>CDI</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>data analyst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5873455</td>\n",
       "      <td>Data Engineer - ETL SAP BO / Logistique (H/F)</td>\n",
       "      <td>Externatic</td>\n",
       "      <td>Annuel de 45000.0 Euros √† 55000.0 Euros</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11 - Castelnaudary</td>\n",
       "      <td>NaN</td>\n",
       "      <td>L'entreprise accompagn√©e et les missions: Vous...</td>\n",
       "      <td>2025-11-26T23:00:52.000Z</td>\n",
       "      <td>5 An(s)</td>\n",
       "      <td>CDI</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11</td>\n",
       "      <td>data engineer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>201QBRP</td>\n",
       "      <td>Data analyst (H/F)</td>\n",
       "      <td>PROMAN</td>\n",
       "      <td>Horaire de 17.0 Euros √† 30.0 Euros sur 12.0 mois</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13 - Marseille 9e Arrondissement</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Le poste : La division IT de PROMAN EXPERTISE ...</td>\n",
       "      <td>2025-12-16T18:00:35.383Z</td>\n",
       "      <td>2 An(s)</td>\n",
       "      <td>MIS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13</td>\n",
       "      <td>data analyst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>201MFRL</td>\n",
       "      <td>Data analyst</td>\n",
       "      <td>DOGG LABEL</td>\n",
       "      <td>Mensuel de 2600.0 Euros sur 12.0 mois</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13 - MARSEILLE 10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>G√©rer et structurer la base de donn√©es clients...</td>\n",
       "      <td>2025-12-12T12:08:58.418Z</td>\n",
       "      <td>24 Mois</td>\n",
       "      <td>CDI</td>\n",
       "      <td>Adapter les outils de traitement statistique d...</td>\n",
       "      <td>13</td>\n",
       "      <td>data analyst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>201JDFR</td>\n",
       "      <td>Data engineer (H/F)</td>\n",
       "      <td>PROXIAD</td>\n",
       "      <td>Annuel de 35000.0 Euros √† 38500.0 Euros sur 12...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13 - AIX EN PROVENCE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Data Engineer exp√©riment√© - Python / GCP / Spa...</td>\n",
       "      <td>2025-12-09T12:38:12.671Z</td>\n",
       "      <td>D√©butant accept√©</td>\n",
       "      <td>CDI</td>\n",
       "      <td>Analyser, exploiter, structurer des donn√©es</td>\n",
       "      <td>13</td>\n",
       "      <td>data engineer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>201WHKH</td>\n",
       "      <td>D√©veloppeur IT - Talend (H/F)</td>\n",
       "      <td>H CONSULTING</td>\n",
       "      <td>Annuel de 36000.0 Euros sur 12.0 mois</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13 - Marseille 8e Arrondissement</td>\n",
       "      <td>NaN</td>\n",
       "      <td>H Consulting recherche un D√©veloppeur IT - Tal...</td>\n",
       "      <td>2025-12-23T14:20:45.567Z</td>\n",
       "      <td>D√©butant accept√©</td>\n",
       "      <td>CDI</td>\n",
       "      <td>Anglais technique|Analyser, exploiter, structu...</td>\n",
       "      <td>13</td>\n",
       "      <td>data engineer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>201NRMG</td>\n",
       "      <td>Ing√©nieur D√©veloppeur PL/SQL (H/F)</td>\n",
       "      <td>GROUPAGORA</td>\n",
       "      <td>Mensuel de 38000.0 Euros √† 42000.0 Euros sur 1...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13 - Aix-en-Provence</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Nous sommes √† la recherche d'un D√©veloppeur PL...</td>\n",
       "      <td>2025-12-15T15:49:51.691Z</td>\n",
       "      <td>3 An(s)</td>\n",
       "      <td>CDI</td>\n",
       "      <td>Anglais technique|Analyser, exploiter, structu...</td>\n",
       "      <td>13</td>\n",
       "      <td>data engineer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id_offre                                              titre  \\\n",
       "0  201STWH  Ing√©nieur.e d'√©tude : traitement de donn√©es/si...   \n",
       "1  201CSRN                                Data Engineer (H/F)   \n",
       "2  6683253                 Charg√© de mission et projets (H/F)   \n",
       "3  3675058                     DATA ANALYST ‚Äì TEMPS PLEIN H/F   \n",
       "4  5873455      Data Engineer - ETL SAP BO / Logistique (H/F)   \n",
       "5  201QBRP                                 Data analyst (H/F)   \n",
       "6  201MFRL                                       Data analyst   \n",
       "7  201JDFR                                Data engineer (H/F)   \n",
       "8  201WHKH                      D√©veloppeur IT - Talend (H/F)   \n",
       "9  201NRMG                 Ing√©nieur D√©veloppeur PL/SQL (H/F)   \n",
       "\n",
       "                                 entreprise  \\\n",
       "0   INRAE CLERMONT-ARA ST-GENES-CHAMPANELLE   \n",
       "1  SOCIETE POUR L'INFORMATIQUE INDUSTRIELLE   \n",
       "2                                       NaN   \n",
       "3                                       NaN   \n",
       "4                                Externatic   \n",
       "5                                    PROMAN   \n",
       "6                                DOGG LABEL   \n",
       "7                                   PROXIAD   \n",
       "8                              H CONSULTING   \n",
       "9                                GROUPAGORA   \n",
       "\n",
       "                                             salaire salaire_complement  \\\n",
       "0             Mensuel de 2244.79 Euros sur 12.0 mois                NaN   \n",
       "1  Mensuel de 37000.0 Euros √† 40000.0 Euros sur 1...                NaN   \n",
       "2            Annuel de 40000.0 Euros √† 50000.0 Euros                NaN   \n",
       "3                                Annuel de 0.0 Euros                NaN   \n",
       "4            Annuel de 45000.0 Euros √† 55000.0 Euros                NaN   \n",
       "5   Horaire de 17.0 Euros √† 30.0 Euros sur 12.0 mois                NaN   \n",
       "6              Mensuel de 2600.0 Euros sur 12.0 mois                NaN   \n",
       "7  Annuel de 35000.0 Euros √† 38500.0 Euros sur 12...                NaN   \n",
       "8              Annuel de 36000.0 Euros sur 12.0 mois                NaN   \n",
       "9  Mensuel de 38000.0 Euros √† 42000.0 Euros sur 1...                NaN   \n",
       "\n",
       "                              ville  region  \\\n",
       "0                    03 - Montoldre     NaN   \n",
       "1                     06 - VALBONNE     NaN   \n",
       "2                     06 - Valbonne     NaN   \n",
       "3             10 - Nogent-sur-Seine     NaN   \n",
       "4                11 - Castelnaudary     NaN   \n",
       "5  13 - Marseille 9e Arrondissement     NaN   \n",
       "6                 13 - MARSEILLE 10     NaN   \n",
       "7              13 - AIX EN PROVENCE     NaN   \n",
       "8  13 - Marseille 8e Arrondissement     NaN   \n",
       "9              13 - Aix-en-Provence     NaN   \n",
       "\n",
       "                                         description  \\\n",
       "0  Vous exercerez votre activit√© au sein de l'Uni...   \n",
       "1  Dans le cadre du d√©veloppement de notre agence...   \n",
       "2  Offre d&apos,emploi : CDI Client final Chef de...   \n",
       "3  Votre mission :   Pr√©parer un plan de travail ...   \n",
       "4  L'entreprise accompagn√©e et les missions: Vous...   \n",
       "5  Le poste : La division IT de PROMAN EXPERTISE ...   \n",
       "6  G√©rer et structurer la base de donn√©es clients...   \n",
       "7  Data Engineer exp√©riment√© - Python / GCP / Spa...   \n",
       "8  H Consulting recherche un D√©veloppeur IT - Tal...   \n",
       "9  Nous sommes √† la recherche d'un D√©veloppeur PL...   \n",
       "\n",
       "           date_publication        experience contrat  \\\n",
       "0  2025-12-19T10:23:24.260Z  D√©butant accept√©     CDD   \n",
       "1  2025-12-03T16:20:41.796Z  D√©butant accept√©     CDI   \n",
       "2  2025-12-17T09:02:07.000Z  D√©butant accept√©     CDI   \n",
       "3  2025-10-11T04:21:38.000Z  D√©butant accept√©     CDI   \n",
       "4  2025-11-26T23:00:52.000Z           5 An(s)     CDI   \n",
       "5  2025-12-16T18:00:35.383Z           2 An(s)     MIS   \n",
       "6  2025-12-12T12:08:58.418Z           24 Mois     CDI   \n",
       "7  2025-12-09T12:38:12.671Z  D√©butant accept√©     CDI   \n",
       "8  2025-12-23T14:20:45.567Z  D√©butant accept√©     CDI   \n",
       "9  2025-12-15T15:49:51.691Z           3 An(s)     CDI   \n",
       "\n",
       "                                         competences departement  \\\n",
       "0        Analyser, exploiter, structurer des donn√©es           3   \n",
       "1        Analyser, exploiter, structurer des donn√©es           6   \n",
       "2                                                NaN           6   \n",
       "3                                                NaN          10   \n",
       "4                                                NaN          11   \n",
       "5                                                NaN          13   \n",
       "6  Adapter les outils de traitement statistique d...          13   \n",
       "7        Analyser, exploiter, structurer des donn√©es          13   \n",
       "8  Anglais technique|Analyser, exploiter, structu...          13   \n",
       "9  Anglais technique|Analyser, exploiter, structu...          13   \n",
       "\n",
       "                     mot_cle_source  \n",
       "0                     data engineer  \n",
       "1                     data engineer  \n",
       "2  consultant business intelligence  \n",
       "3                      data analyst  \n",
       "4                     data engineer  \n",
       "5                      data analyst  \n",
       "6                      data analyst  \n",
       "7                     data engineer  \n",
       "8                     data engineer  \n",
       "9                     data engineer  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"C:/Users/Etu/Desktop/api_domaine_info.csv\",  encoding = \"utf-8\") \n",
    "df.info() \n",
    "df[\"titre\"] \n",
    "df.head(10) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac9c9a5-9a67-494c-96bd-9bdd5c76e422",
   "metadata": {},
   "source": [
    "## suppression des doublons\r",
    "supprimons les doublons en nous basant sur le titre, l‚Äôentreprise et le salaire \n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e15b6538-641c-4629-a2db-ac5218fafb79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre total de lignes : 956\n"
     ]
    }
   ],
   "source": [
    "df = df.drop_duplicates(subset=[\"titre\", \"entreprise\", \"salaire_texte_brut\"], keep=\"first\")\n",
    "#sauvgarde du fichier sans les doublon \n",
    "df.to_csv(\"C:/Users/Etu/Desktop/no_duplicates_informatique.csv\", encoding= \"utf-8-sig\",index = False)\n",
    "print(\"Nombre total de lignes :\", len(df))  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa098b18-bbc9-4031-b5eb-b2c194d07401",
   "metadata": {},
   "source": [
    "##  Nettoyage des textes\r\n",
    "Les colonnes `title` et `description` contiennent des caract√®res sp√©ciaux, \r\n",
    "des majuscules et des √©l√©ments inutile‚Äù).  \r\n",
    "Nous nettoyons ces textes pour faciliter l‚Äôanalyse linguistique.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f7cf2995-5132-40ab-b3da-2776f8b2ca04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re  \n",
    "df = pd.read_csv(\"C:/Users/Etu/Desktop/api_domaine_info.csv\",  encoding = \"utf-8\") \n",
    "df[\"titre\"] = df[\"titre\"].apply(lambda x: re.sub(r\"H/F.*$\", \"\", str(x))).str.lower() \n",
    "df[\"titre\"] = df[\"titre\"].apply(lambda x: re.sub(r\"H/F.*$\", \"\", str(x))).str.lower() \n",
    "df['titre'] = df['titre'].str.replace('\\n', ' ', regex=False) \n",
    "df['titre'] = df['titre'].str.replace('H/F', ' ', regex=False)  \n",
    "df['titre'] = df['titre'].str.replace('(H/F)', ' ', regex=False) \n",
    "df['titre'] = df['titre'].str.replace('(F/H)', ' ', regex=False)\n",
    "df['titre'] = df['titre'].str.replace('F/H', ' ', regex=False)\n",
    "\n",
    "df[\"description\"] = df[\"description\"].str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
    "  \n",
    "df[\"description\"] = df[\"description\"].apply(lambda x: re.sub(r\"[^a-zA-Z√Ä-√ø0-9\\s]\", \" \", str(x))).str.lower() \n",
    "df[\"titre\"].head(20)  \n",
    "\n",
    "\n",
    "df.to_csv(\"C:/Users/Etu/Desktop/api_domaine_infoo.csv\",  encoding = \"utf-8-sig\",index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d273bf1a-b9a2-42c4-b3de-c66d604ecf4c",
   "metadata": {},
   "source": [
    "On constate egalement des carat√®re \"\\n\" issue des retoure a ligne dans les text description , on va enlever les caract√®re \n",
    "puis normaliser les espace pour rendre le "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "31097cca-33a5-452d-a546-4486af4f2879",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"C:/Users/Etu/Desktop/api_domaine_info.csv\",  encoding = \"utf-8\") \n",
    "df['description'] = df['description'].str.replace('\\n', ' ', regex=False)\n",
    "df[\"description\"] = df[\"description\"].str.replace(r\"\\s+\", \" \", regex=True).str.strip() # normalise les espace  \n",
    "df[\"description\"] = df[\"description\"].str.replace(r\",\", \" \", regex=True).str.strip() # normalise les espace  \n",
    "df[\"description\"].iloc[677] \n",
    "\n",
    "df.to_csv(\"C:/Users/Etu/Desktop/api_domaine_infoo.csv\",  encoding = \"utf-8-sig\",index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de87d18-c6ea-4f33-80b9-dcb3d4c1ebb1",
   "metadata": {},
   "source": [
    "nettoyage des salaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1257531e-1a98-4cad-839f-cc80774c806e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Calcul en cours...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"C:/Users/Etu/Desktop/api_domaine_info.csv\",  encoding = \"utf-8\")\n",
    "\n",
    "\n",
    "# 2. LA FONCTION DE NETTOYAGE\n",
    "def nettoyer_salaire_intelligent(texte):\n",
    "    if pd.isna(texte) or texte == \"\": return None, None, None\n",
    "    texte_clean = str(texte).lower().replace(\" \", \"\").replace(\",\", \".\")\n",
    "    \n",
    "    matches = re.findall(r\"(\\d+(?:\\.\\d+)?)\", texte_clean)\n",
    "    if not matches: return None, None, None\n",
    "\n",
    "    est_mensuel = \"mensuel\" in texte_clean or \"mois\" in texte_clean\n",
    "    est_horaire = \"heure\" in texte_clean or \"/h\" in texte_clean\n",
    "\n",
    "    valeurs_annuelles = []\n",
    "    for m in matches:\n",
    "        try:\n",
    "            val = float(m)\n",
    "            if est_horaire: annuel = val * 1820\n",
    "            elif est_mensuel:\n",
    "                if val > 12000: annuel = val # Erreur saisie, d√©j√† annuel\n",
    "                else: annuel = val * 12\n",
    "            else: annuel = val # D√©j√† annuel\n",
    "\n",
    "            # Filtre de r√©alisme (15k √† 200k)\n",
    "            if 15000 <= annuel <= 200000:\n",
    "                valeurs_annuelles.append(annuel)\n",
    "        except: pass\n",
    "\n",
    "    if not valeurs_annuelles: return None, None, None\n",
    "    \n",
    "    valeurs_annuelles.sort()\n",
    "    return valeurs_annuelles[0], valeurs_annuelles[-1], sum(valeurs_annuelles)/len(valeurs_annuelles)\n",
    "\n",
    "# 3. CR√âATION DES COLONNES (C'est l'√©tape cruciale !)\n",
    "print(\"üîÑ Calcul en cours...\")\n",
    "df[['salaire_min_final', 'salaire_max_final', 'salaire_avg_final']] = df['salaire'].apply(\n",
    "    lambda x: pd.Series(nettoyer_salaire_intelligent(x))\n",
    ")\n",
    "\n",
    "\n",
    "df.to_csv(\"C:/Users/Etu/Desktop/api_domaine_infoo.csv\",  encoding = \"utf-8-sig\",index = False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f699a05c-457f-4533-89e5-28ce93eb21bc",
   "metadata": {},
   "source": [
    "# suprimon les lignes qui ne contienne pas de salaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "75fd9b37-58f8-4a4a-a309-aae9722134e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avant: 956\n",
      "apres : 913\n"
     ]
    }
   ],
   "source": [
    "mask = df[\"salaire_texte_brut\"].str.contains(r\"Non renseign√©\",case = False,na=False) \n",
    "df_filtrer = df[~mask] \n",
    "print(\"Avant:\",len(df))\n",
    "print(\"apres :\", len(df_filtrer)) \n",
    "#sauvgarder\n",
    "df_filtrer.to_csv(\"C:/Users/Etu/Desktop/filtrer_domaine_info.csv\", encoding= \"utf-8-sig\",index = False) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ed1e4337-7870-409e-bb13-bfbf8ebde383",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      32 000 - 35 000 ‚Ç¨\n",
       "1      28 000 - 30 000 ‚Ç¨\n",
       "2               34 000 ‚Ç¨\n",
       "3      40 000 - 80 000 ‚Ç¨\n",
       "4                  2 134\n",
       "             ...        \n",
       "908             50 700 ‚Ç¨\n",
       "909    34 000 - 39 000 ‚Ç¨\n",
       "910    55 000 - 65 000 ‚Ç¨\n",
       "911    40 000 - 45 000 ‚Ç¨\n",
       "912    30 000 - 40 000 ‚Ç¨\n",
       "Name: salaire_texte_brut, Length: 913, dtype: object"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"C:/Users/Etu/Desktop/informatique.csv\", encoding= \"utf-8\") \n",
    "df[\"salaire_texte_brut\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4cd898-30ce-46c9-8f5a-3ef8f9115534",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e53fba51-d8a8-4438-acef-0fe7512e0691",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "837"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import re \n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def parse_salary(s):\n",
    "     if pd.isna(s):\n",
    "        return pd.Series({\n",
    "            \"salary_min\": np.nan,\n",
    "            \"salary_max\": np.nan,\n",
    "            \"avg_salary\": np.nan\n",
    "        })\n",
    "\n",
    "     s = str(s).lower()\n",
    "\n",
    "    # d√©tecter la p√©riode\n",
    "   \n",
    "     is_monthly = any(x in s for x in [\"mois\", \"mensuel\", \"month\"])\n",
    "     is_yearly = any(x in s for x in [\"‚Ç¨ / an\"])\n",
    "\n",
    "\n",
    "    # extraire les nombres (virgules, espaces, milliers)\n",
    "     numbers = re.findall(\n",
    "        r\"\\d{1,3}(?:[ .]\\d{3})*(?:,\\d+)?|\\d+(?:,\\d+)?\",\n",
    "        s\n",
    "     )\n",
    "\n",
    "     values = []\n",
    "     for n in numbers:\n",
    "        n = n.replace(\" \", \"\").replace(\".\", \"\").replace(\",\", \".\")\n",
    "        try:\n",
    "            values.append(float(n))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "     if not values:\n",
    "        return pd.Series([np.nan, np.nan, np.nan])\n",
    "\n",
    "    # min / max\n",
    "     if len(values) == 1:\n",
    "        salary_max = values[0]\n",
    "        salary_min = salary_max * 0.85\n",
    "     else:\n",
    "        salary_min = min(values)\n",
    "        salary_max = max(values)\n",
    "\n",
    "\n",
    "     if is_monthly :\n",
    "        salary_min *= 12\n",
    "        salary_max *= 12\n",
    "\n",
    "     avg_salary = (salary_min + salary_max) / 2\n",
    "\n",
    "     return pd.Series(\n",
    "    {\n",
    "        \"salary_min\": salary_min,\n",
    "        \"salary_max\": salary_max,\n",
    "        \"avg_salary\": avg_salary\n",
    "    }\n",
    ")\n",
    "\n",
    "# apply to dataframe\n",
    "df[[\"salary_min\", \"salary_max\", \"avg_salary\"]] = df[\"salaire_texte_brut\"].apply(parse_salary) \n",
    "\n",
    "df = df[(df[\"avg_salary\"] >= 12000) & (df[\"avg_salary\"] <= 300000)]\n",
    "#sauvgardon \n",
    "df.to_csv(\"C:/Users/Etu/Desktop/cleaned_info.csv\", encoding= \"utf-8-sig\",index= False)\n",
    "len(df)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4a12bd-ed1c-449f-ba60-8a12093b17da",
   "metadata": {},
   "source": [
    "## Extraction du Niveau d'Exp√©rience\n",
    "\n",
    "**Objectif :**\n",
    "Cr√©er une nouvelle variable cat√©gorielle `experience_finale` pour am√©liorer la pr√©cision du mod√®le.\n",
    "\n",
    "**Probl√©matique :**\n",
    "L'exp√©rience est souvent manquante ou mal structur√©e dans les donn√©es brutes. \n",
    "\n",
    "La fonction `get_smart_experience` analyse le texte avec une logique de priorit√© d√©croissante :\n",
    "\n",
    "1.  **Analyse du TITRE (Priorit√© Haute)** \n",
    "    *   Le titre est l'indicateur le plus fiable pour les contrats sp√©cifiques.\n",
    "    *   *D√©tection :* \"Stage\", \"Alternance\", \"Lead\", \"Senior\", \"CTO\", \"Manager\".\n",
    "\n",
    "2.  **Extraction Num√©rique (Regex sur Description)** \n",
    "    *   Recherche de motifs temporels explicites dans le texte (ex: *\"5 ans d'exp√©rience\"*, *\"2 ann√©es\"*).\n",
    "    *   **Classification automatique :**\n",
    "        *   `0 - 2 ans` $\\rightarrow$ **Junior**\n",
    "        *   `3 - 5 ans` $\\rightarrow$ **Interm√©diaire**\n",
    "        *   `> 5 ans` $\\rightarrow$ **Senior**\n",
    "\n",
    "3.  **Analyse S√©mantique (Mots-cl√©s)** \n",
    "    *   Si aucun chiffre n'est trouv√©, recherche de termes qualitatifs.\n",
    "    *   *Exemples :* \"D√©butant accept√©\", \"Confirm√©\", \"Expert\".\n",
    "\n",
    "4.  **Cas par d√©faut**\n",
    "    *   Si aucune information n'est d√©tect√©e, la valeur est fix√©e √† **\"Non sp√©cifi√©\"**. Cette cat√©gorie sera trait√©e par le `OneHotEncoder` comme une information √† part enti√®re (souvent corr√©l√©e √† un salaire standard)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "ef355d20-bbfb-47ab-a420-5322950b9a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Extraction intelligente de l'exp√©rience...\n",
      "\n",
      " R√©partition des niveaux d'exp√©rience :\n",
      "experience_finale\n",
      "Senior (5+ ans)            292\n",
      "Interm√©diaire (2-5 ans)    263\n",
      "Junior (0-2 ans)           168\n",
      "Non sp√©cifi√©               113\n",
      "Stage                        1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Charge ton fichier\n",
    "df = pd.read_csv(\"C:/Users/Etu/Desktop/informatique.csv\")\n",
    "\n",
    "def get_smart_experience(row):\n",
    "    # On combine Titre + Description pour chercher l'info\n",
    "    # On donne la priorit√© au Titre \n",
    "    titre = str(row.get('titre', '')).lower()\n",
    "    desc = str(row.get('description', '')).lower()\n",
    "    \n",
    "    # 1. ANALYSE DU TITRE (Priorit√© Absolue)\n",
    "    if \"stage\" in titre: return \"Stage\"\n",
    "    if \"alternance\" in titre or \"apprenti\" in titre: return \"Alternance\"\n",
    "    if \"senior\" in titre or \"lead\" in titre or \"architecte\" in titre or \"directeur\" in titre or \"cto\" in titre or \"manager\" in titre:\n",
    "        return \"Senior (5+ ans)\"\n",
    "    if \"junior\" in titre: return \"Junior (0-2 ans)\"\n",
    "\n",
    "    # Recherche de chiffres pr√©cis : \"5 ans\", \"2 ann√©es\n",
    "    match = re.search(r\"(\\d{1,2})\\s*(?:ans|ann[√©e]es)\", desc)\n",
    "    if match:\n",
    "        years = int(match.group(1))\n",
    "        if years == 0: return \"Junior (0-2 ans)\"\n",
    "        if 1 <= years <= 2: return \"Junior (0-2 ans)\"\n",
    "        if 3 <= years <= 5: return \"Interm√©diaire (2-5 ans)\"\n",
    "        if years > 5: return \"Senior (5+ ans)\"\n",
    "\n",
    "    # Mots cl√©s dans la description\n",
    "    if \"d√©butant accept√©\" in desc: return \"Junior (0-2 ans)\"\n",
    "    if \"premi√®re exp√©rience\" in desc: return \"Junior (0-2 ans)\"\n",
    "    if \"exp√©riment√©\" in desc or \"expert\" in desc: return \"Senior (5+ ans)\"\n",
    "    if \"confirm√©\" in desc: return \"Interm√©diaire (2-5 ans)\"  \n",
    "    if \"senior\" in desc or \"lead\" in desc or \"architecte\" in desc or \"directeur\" in desc or \"cto\" in desc or \"manager\" in desc:\n",
    "        return \"Senior (5+ ans)\"\n",
    "        \n",
    "\n",
    "    # si rien n'est trouver\n",
    "    return \"Non sp√©cifi√©\"\n",
    "\n",
    "# Application ligne par ligne (axis=1 permet de lire toute la ligne)\n",
    "print(\" Extraction intelligente de l'exp√©rience...\")\n",
    "df['experience_finale'] = df.apply(get_smart_experience, axis=1)\n",
    "\n",
    "# V√©rification des r√©sultats\n",
    "print(\"\\n R√©partition des niveaux d'exp√©rience :\")\n",
    "print(df['experience_finale'].value_counts())\n",
    "\n",
    "# Sauvegarde\n",
    "df.to_csv(\"C:/Users/Etu/Desktop/dataset_ready_v2.csv\", index=False, encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "77f191ac-6946-42d3-bcbd-2a0d977ee14d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2303"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5d516d6c-6dc2-4447-ba14-632bf349e981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Extraction PRO des comp√©tences (Titre + Description)...\n",
      "                  competences\n",
      "0                       agile\n",
      "1                         sap\n",
      "2           power bi, tableau\n",
      "3              git, github, r\n",
      "4                sql, tableau\n",
      "5  power bi, python, sql, vba\n",
      "6       etl, power bi, python\n",
      "7          sql, tableau, vite\n",
      "8          sql, tableau, vite\n",
      "9                 python, sql\n",
      "‚úÖ Fichier sauvegard√© avec succ√®s !\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Charge ton fichier\n",
    "df = pd.read_csv(\"C:/Users/Etu/Desktop/cleaned_info.csv\") \n",
    "\n",
    "# --- LA BIBLE DES COMP√âTENCES IT (Liste √âtendue) ---\n",
    "TECH_KEYWORDS = [\n",
    "    # ‚û§ Langages de programmation\n",
    "    \"python\", \"java\", \"javascript\", \"typescript\", \"c++\", \"c#\", \"php\", \"ruby\", \"go\", \"golang\", \n",
    "    \"rust\", \"swift\", \"kotlin\", \"scala\", \"r\", \"vba\", \"sql\", \"pl/sql\", \"cobol\", \"abap\", \"perl\", \"bash\", \"shell\", \"django\",\n",
    "    \n",
    "    # ‚û§ Web Front (Frameworks & Outils)\n",
    "    \"react\", \"react.js\", \"angular\", \"vue\", \"vue.js\", \"node\", \"node.js\", \"next.js\", \"nuxt.js\", \"svelte\",\n",
    "    \"html\", \"css\", \"sass\", \"less\", \"tailwind\", \"bootstrap\", \"jquery\", \"webpack\", \"vite\",\n",
    "    \n",
    "    # ‚û§ Web Back & API\n",
    "    \"django\", \"flask\", \"fastapi\", \"spring\", \"spring boot\", \"symfony\", \"laravel\", \"dotnet\", \".net\", \"asp.net\",\n",
    "    \"hibernate\", \"jpa\", \"graphql\", \"rest api\", \"soap\",\n",
    "    \n",
    "    # ‚û§ Data, Big Data & IA\n",
    "    \"pandas\", \"numpy\", \"scikit-learn\", \"tensorflow\", \"pytorch\", \"keras\", \"opencv\", \"llm\", \"transformers\",\n",
    "    \"spark\", \"pyspark\", \"hadoop\", \"hive\", \"kafka\", \"flink\", \"airflow\", \"dbt\", \"databricks\", \"snowflake\",\n",
    "    \"tableau\", \"power bi\", \"qlik\", \"talend\", \"ssis\", \"ssas\", \"ssrs\", \"nlp\", \"etl\",\n",
    "    \n",
    "    # ‚û§ Cloud & DevOps & Infra\n",
    "    \"aws\", \"amazon web services\", \"azure\", \"gcp\", \"google cloud\", \"docker\", \"kubernetes\", \"k8s\", \n",
    "    \"ansible\", \"terraform\", \"jenkins\", \"gitlab ci\", \"github actions\", \"circleci\", \"prometheus\", \"grafana\", \n",
    "    \"elk\", \"elastic\", \"linux\", \"unix\", \"ubuntu\", \"redhat\", \"centos\", \"debian\", \"vmware\", \"hyper-v\",\n",
    "    \n",
    "    # ‚û§ Bases de donn√©es (SQL & NoSQL)\n",
    "    \"mysql\", \"postgresql\", \"postgres\", \"oracle\", \"sql server\", \"mariadb\", \"sqlite\",\n",
    "    \"mongodb\", \"cassandra\", \"redis\", \"elasticsearch\", \"neo4j\", \"dynamodb\", \"cosmos db\",\n",
    "    \n",
    "    # ‚û§ Mobile\n",
    "    \"react native\", \"flutter\", \"ios\", \"android\", \"xamarin\", \"ionic\", \"dart\",\n",
    "    \n",
    "    # ‚û§ Gestion de projet, M√©thodes & Outils\n",
    "    \"agile\", \"scrum\", \"kanban\", \"safe\", \"jira\", \"confluence\", \"git\", \"github\", \"gitlab\", \"svn\", \"trello\",\n",
    "    \"uml\", \"merise\",\n",
    "    \n",
    "    # ‚û§ CMS & E-commerce\n",
    "    \"wordpress\", \"drupal\", \"joomla\", \"magento\", \"prestashop\", \"shopify\", \"salesforce\", \"sap\", \"odoo\",\n",
    "    \n",
    "    # ‚û§ Cybers√©curit√© & R√©seau\n",
    "    \"owasp\", \"pci-dss\", \"gdpr\", \"rgpd\", \"vpn\", \"firewall\", \"cisco\", \"fortinet\", \"wireshark\", \"kali\",\n",
    "    \n",
    "  \n",
    "]\n",
    "\n",
    "def extract_skills_extended(row):\n",
    "    # 1. On fusionne Titre + Description pour ne rien rater\n",
    "    # On convertit en string pour √©viter les bugs si une case est vide\n",
    "    titre = str(row.get('titre', ''))\n",
    "    desc = str(row.get('description', '')) # ou 'description' selon ton fichier\n",
    "    \n",
    "    # On met tout en minuscule\n",
    "    full_text = (titre + \" \" + desc).lower()\n",
    "    \n",
    "    found_skills = set() # 'set' emp√™che les doublons automatiquement\n",
    "    \n",
    "    for skill in TECH_KEYWORDS:\n",
    "        # Regex \\b pour mot entier (√©vite de trouver \"java\" dans \"javascript\")\n",
    "        pattern = r\"\\b\" + re.escape(skill) + r\"\\b\"\n",
    "        \n",
    "        if re.search(pattern, full_text):\n",
    "            # Cas particulier : harmonisation\n",
    "            if skill in [\"node.js\", \"node\"]: found_skills.add(\"node.js\")\n",
    "            elif skill in [\"react.js\", \"react\"]: found_skills.add(\"react\")\n",
    "            elif skill in [\"vue.js\", \"vue\"]: found_skills.add(\"vue.js\")\n",
    "            elif skill in [\"aws\", \"amazon web services\"]: found_skills.add(\"aws\")\n",
    "            elif skill in [\"golang\", \"go\"]: found_skills.add(\"go\")\n",
    "            else:\n",
    "                found_skills.add(skill)\n",
    "            \n",
    "    if found_skills:\n",
    "        return \", \".join(sorted(list(found_skills))) # On trie par ordre alphab√©tique\n",
    "    return \"Non sp√©cifi√©\"\n",
    "\n",
    "print(\" Extraction PRO des comp√©tences (Titre + Description)...\")\n",
    "\n",
    "# Application sur tout le DataFrame\n",
    "df['competences'] = df.apply(extract_skills_extended, axis=1)\n",
    "\n",
    "# Aper√ßu\n",
    "print(df[['competences']].head(10))\n",
    "\n",
    "#Sauvegarde\n",
    "df.to_csv(\"C:/Users/Etu/Desktop/informatique_FINAL_SKILLS.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "print(\"‚úÖ Fichier sauvegard√© avec succ√®s !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8b4b1d1f-9bf2-4b67-8f8c-129f52824ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "competences\n",
      "Non sp√©cifi√©                                                                        884\n",
      "sql                                                                                  54\n",
      "agile                                                                                36\n",
      "linux                                                                                33\n",
      "r                                                                                    30\n",
      "                                                                                   ... \n",
      ".net, asp.net, css, javascript, jquery, oracle, r, sql, ssas, ssis, ssrs, vue.js      1\n",
      "tableau                                                                               1\n",
      "airflow, etl, git, mysql, pandas, python, rgpd, sql                                   1\n",
      "airflow, aws, azure, etl, gcp, mysql, pyspark, python                                 1\n",
      "airflow, aws, azure, etl, gcp, python, spark, sql                                     1\n",
      "Name: count, Length: 1068, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"C:/Users/Etu/Desktop/informatique_FINAL_SKILLS.csv\",  encoding=\"utf-8\") \n",
    "print(df['competences'].value_counts()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2a275203-567f-4dc9-acb1-024f11f5193e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Extraction des D√©partements...\n",
      "                    ville departement\n",
      "0              Paris - 75          75\n",
      "1   Champs-sur-Marne - 77          77\n",
      "2              Paris - 75          75\n",
      "3  Neuilly-sur-Seine - 92          92\n",
      "4    Rueil-Malmaison - 92          92\n",
      "5            Puteaux - 92          92\n",
      "6     Roissy-en-Brie - 77          77\n",
      "7              Paris - 75          75\n",
      "8            Provins - 77          77\n",
      "9              Paris - 75          75\n",
      " Fichier sauvegard√© avec la colonne 'departement' !\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# 1. Charge ton fichier (celui avec les comp√©tences)\n",
    "df = pd.read_csv(\"C:/Users/Etu/Desktop/cleaned_info.csv\") \n",
    "\n",
    "def extract_department(text):\n",
    "    if pd.isna(text) or text == \"\": return \"Non sp√©cifi√©\"\n",
    "    text = str(text).upper() # Majuscule pour g√©rer la Corse (2A/2B)\n",
    "    \n",
    "    # 1. Gestion Sp√©ciale CORSE (2A et 2B)\n",
    "    if \"2A\" in text: return \"2A\"\n",
    "    if \"2B\" in text: return \"2B\"\n",
    "    \n",
    "    # 2. Regex pour trouver le Code\n",
    "    # On cherche 2 ou 3 chiffres (pour les DOM 971...)\n",
    "    # - \\b : d√©but de mot\n",
    "    # - (97\\d|\\d{2}) : Soit un DOM (97x), soit 2 chiffres classiques\n",
    "    # - (?:\\d{3})? : Optionnellement suivi de 3 chiffres (si c'est un code postal complet ex: 75001)\n",
    "    match = re.search(r\"\\b(97\\d|\\d{2})(?:\\d{3})?\\b\", text)\n",
    "    \n",
    "    if match:\n",
    "        # On ne garde que le groupe captur√© (les 2 ou 3 premiers chiffres)\n",
    "        dept = match.group(1)\n",
    "        \n",
    "        # Filtre de s√©curit√© : un d√©partement est entre 01 et 98\n",
    "        # (√âvite de prendre \"Bac +5\" comme d√©partement 05)\n",
    "        if dept.isdigit():\n",
    "            val = int(dept)\n",
    "            if 1 <= val <= 95 or val > 970:\n",
    "                return dept\n",
    "            \n",
    "    return \"Non sp√©cifi√©\"\n",
    "\n",
    "print(\" Extraction des D√©partements...\")\n",
    "\n",
    "# Application\n",
    "df['departement'] = df['ville'].apply(extract_department)\n",
    "\n",
    "# V√©rification : Regardons les r√©sultats\n",
    "print(df[['ville', 'departement']].head(10))\n",
    "\n",
    "# Sauvegarde\n",
    "df.to_csv(\"C:/Users/Etu/Desktop/clean_detaprtement.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "print(\" Fichier sauvegard√© avec la colonne 'departement' !\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "af53ef09-9b85-4f65-bcab-b9c53119a69c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1432"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"C:/Users/Etu/Desktop/noo_duplicated_informatique.csv\",encoding = \"utf-8\") \n",
    "df.drop_duplicates(subset = [\"titre\",\"entreprise\",\"salaire_texte_brut\"]).sum() \n",
    "df.to_csv(\"C:/Users/Etu/Desktop/noo0_duplicated_informatique.csv\",encoding = \"utf-8-sig\",index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a2c2975c-7e7a-4013-b889-42f11feff0b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>titre</th>\n",
       "      <th>entreprise</th>\n",
       "      <th>salaire_texte_brut</th>\n",
       "      <th>ville</th>\n",
       "      <th>description</th>\n",
       "      <th>region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.hellowork.com/fr-fr/emplois/729729...</td>\n",
       "      <td>Charg√© Informatique H/F IONIS Education Group</td>\n",
       "      <td>IONIS Education Group</td>\n",
       "      <td>32 000 - 35 000 ‚Ç¨</td>\n",
       "      <td>Paris 3e - 75</td>\n",
       "      <td>Le/la Charg√©(e) informatique assure la gestion...</td>\n",
       "      <td>√éle-de-France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.hellowork.com/fr-fr/emplois/733563...</td>\n",
       "      <td>Gestionnaire d'Habilitations Informatique H/F ...</td>\n",
       "      <td>AS International</td>\n",
       "      <td>28 000 - 30 000 ‚Ç¨</td>\n",
       "      <td>Nanterre - 92</td>\n",
       "      <td>Nous recherchons un Gestionnaire d'habilitatio...</td>\n",
       "      <td>√éle-de-France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.hellowork.com/fr-fr/emplois/729798...</td>\n",
       "      <td>Directeur des Op√©rations Informatique H/F Orev...</td>\n",
       "      <td>Oreve - Ortec Group</td>\n",
       "      <td>34 000 ‚Ç¨</td>\n",
       "      <td>Paris - 75</td>\n",
       "      <td>Obornes, repr√©sent√© par la marque Oreve, est l...</td>\n",
       "      <td>√éle-de-France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.hellowork.com/fr-fr/emplois/739609...</td>\n",
       "      <td>Ing√©nieur Commercial Informatique H/F DFM</td>\n",
       "      <td>DFM</td>\n",
       "      <td>40 000 - 80 000 ‚Ç¨</td>\n",
       "      <td>Cr√©teil - 94</td>\n",
       "      <td>Rejoignez DFM, entreprise certifi√©e Great Plac...</td>\n",
       "      <td>√éle-de-France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.hellowork.com/fr-fr/emplois/742352...</td>\n",
       "      <td>Sp√©cialiste R√©seaux Informatiques H/F Arm√©e de...</td>\n",
       "      <td>Arm√©e de Terre</td>\n",
       "      <td>134 ‚Ç¨</td>\n",
       "      <td>Versailles - 78</td>\n",
       "      <td>VOTRE MISSION En tant que sp√©cialiste r√©seaux ...</td>\n",
       "      <td>√éle-de-France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1427</th>\n",
       "      <td>https://www.hellowork.com/fr-fr/emplois/429682...</td>\n",
       "      <td>Auditeur - Consultant Si Senior - le Havre H/F...</td>\n",
       "      <td>Forvis Mazars</td>\n",
       "      <td>50 700 ‚Ç¨</td>\n",
       "      <td>Le Havre - 76</td>\n",
       "      <td>Forvis Mazars recrute au sein de son bureau ha...</td>\n",
       "      <td>Normandie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1428</th>\n",
       "      <td>https://www.hellowork.com/fr-fr/emplois/698423...</td>\n",
       "      <td>Consultant Power Bi H/F Proxiad</td>\n",
       "      <td>Proxiad</td>\n",
       "      <td>34 000 - 39 000 ‚Ç¨</td>\n",
       "      <td>Rouen - 76</td>\n",
       "      <td>A propos de Proxiad Axe Seine Proxiad Axe Sein...</td>\n",
       "      <td>Normandie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1429</th>\n",
       "      <td>https://www.hellowork.com/fr-fr/emplois/734138...</td>\n",
       "      <td>Chef de Projets IT H/F Approach People Recruit...</td>\n",
       "      <td>Approach People Recruitment</td>\n",
       "      <td>55 000 - 65 000 ‚Ç¨</td>\n",
       "      <td>Rouen - 76</td>\n",
       "      <td>Chef de Projet IT : Localisation : Rouen (d√©pl...</td>\n",
       "      <td>Normandie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1430</th>\n",
       "      <td>https://www.hellowork.com/fr-fr/emplois/733993...</td>\n",
       "      <td>Analyste COBOL Confirm√© H/F Step Up</td>\n",
       "      <td>Step Up</td>\n",
       "      <td>40 000 - 45 000 ‚Ç¨</td>\n",
       "      <td>Rouen - 76</td>\n",
       "      <td>Pour l'un de nos clients dans le domaine des a...</td>\n",
       "      <td>Normandie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1431</th>\n",
       "      <td>https://www.hellowork.com/fr-fr/emplois/733993...</td>\n",
       "      <td>Ing√©nieur Validation Syst√®me Adas - Essais Ter...</td>\n",
       "      <td>Step Up</td>\n",
       "      <td>30 000 - 40 000 ‚Ç¨</td>\n",
       "      <td>Eure - 27</td>\n",
       "      <td>Ing√©nieur Validation Syst√®me ADAS - Essais ter...</td>\n",
       "      <td>Normandie</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1432 rows √ó 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    url  \\\n",
       "0     https://www.hellowork.com/fr-fr/emplois/729729...   \n",
       "1     https://www.hellowork.com/fr-fr/emplois/733563...   \n",
       "2     https://www.hellowork.com/fr-fr/emplois/729798...   \n",
       "3     https://www.hellowork.com/fr-fr/emplois/739609...   \n",
       "4     https://www.hellowork.com/fr-fr/emplois/742352...   \n",
       "...                                                 ...   \n",
       "1427  https://www.hellowork.com/fr-fr/emplois/429682...   \n",
       "1428  https://www.hellowork.com/fr-fr/emplois/698423...   \n",
       "1429  https://www.hellowork.com/fr-fr/emplois/734138...   \n",
       "1430  https://www.hellowork.com/fr-fr/emplois/733993...   \n",
       "1431  https://www.hellowork.com/fr-fr/emplois/733993...   \n",
       "\n",
       "                                                  titre  \\\n",
       "0         Charg√© Informatique H/F IONIS Education Group   \n",
       "1     Gestionnaire d'Habilitations Informatique H/F ...   \n",
       "2     Directeur des Op√©rations Informatique H/F Orev...   \n",
       "3             Ing√©nieur Commercial Informatique H/F DFM   \n",
       "4     Sp√©cialiste R√©seaux Informatiques H/F Arm√©e de...   \n",
       "...                                                 ...   \n",
       "1427  Auditeur - Consultant Si Senior - le Havre H/F...   \n",
       "1428                    Consultant Power Bi H/F Proxiad   \n",
       "1429  Chef de Projets IT H/F Approach People Recruit...   \n",
       "1430                Analyste COBOL Confirm√© H/F Step Up   \n",
       "1431  Ing√©nieur Validation Syst√®me Adas - Essais Ter...   \n",
       "\n",
       "                       entreprise salaire_texte_brut            ville  \\\n",
       "0           IONIS Education Group  32 000 - 35 000 ‚Ç¨    Paris 3e - 75   \n",
       "1                AS International  28 000 - 30 000 ‚Ç¨    Nanterre - 92   \n",
       "2             Oreve - Ortec Group           34 000 ‚Ç¨       Paris - 75   \n",
       "3                             DFM  40 000 - 80 000 ‚Ç¨     Cr√©teil - 94   \n",
       "4                  Arm√©e de Terre              134 ‚Ç¨  Versailles - 78   \n",
       "...                           ...                ...              ...   \n",
       "1427                Forvis Mazars           50 700 ‚Ç¨    Le Havre - 76   \n",
       "1428                      Proxiad  34 000 - 39 000 ‚Ç¨       Rouen - 76   \n",
       "1429  Approach People Recruitment  55 000 - 65 000 ‚Ç¨       Rouen - 76   \n",
       "1430                      Step Up  40 000 - 45 000 ‚Ç¨       Rouen - 76   \n",
       "1431                      Step Up  30 000 - 40 000 ‚Ç¨        Eure - 27   \n",
       "\n",
       "                                            description         region  \n",
       "0     Le/la Charg√©(e) informatique assure la gestion...  √éle-de-France  \n",
       "1     Nous recherchons un Gestionnaire d'habilitatio...  √éle-de-France  \n",
       "2     Obornes, repr√©sent√© par la marque Oreve, est l...  √éle-de-France  \n",
       "3     Rejoignez DFM, entreprise certifi√©e Great Plac...  √éle-de-France  \n",
       "4     VOTRE MISSION En tant que sp√©cialiste r√©seaux ...  √éle-de-France  \n",
       "...                                                 ...            ...  \n",
       "1427  Forvis Mazars recrute au sein de son bureau ha...      Normandie  \n",
       "1428  A propos de Proxiad Axe Seine Proxiad Axe Sein...      Normandie  \n",
       "1429  Chef de Projet IT : Localisation : Rouen (d√©pl...      Normandie  \n",
       "1430  Pour l'un de nos clients dans le domaine des a...      Normandie  \n",
       "1431  Ing√©nieur Validation Syst√®me ADAS - Essais ter...      Normandie  \n",
       "\n",
       "[1432 rows x 7 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "5df6402f-4ee6-4ad7-ba96-d56cc0c5b91b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3067"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "\n",
    "\n",
    "df1 = pd.read_csv(\"C:/Users/Etu/Desktop/data_jobs_WITH_SALARY.csv\" ,  encoding = \"utf-8\") \n",
    "df2 = pd.read_csv(\"C:/Users/Etu/Desktop/cloud_WITH_SALARY.csv\" ,  encoding = \"utf-8\") \n",
    "df3 = pd.read_csv(\"C:/Users/Etu/Desktop/developeur_WITH_SALARY.csv\" ,  encoding = \"utf-8\")\n",
    "df4 = pd.read_csv(\"C:/Users/Etu/Desktop/cybercycurite_WITH_SALARY.csv\" ,  encoding = \"utf-8\")\n",
    "df5 = pd.read_csv(\"C:/Users/Etu/Desktop/filtered_web_WITH_SALARY.csv\" ,  encoding = \"utf-8\")\n",
    "df6 = pd.read_csv(\"C:/Users/Etu/Desktop/chf_projet_WITH_SALARY.csv\" ,  encoding = \"utf-8\")\n",
    "\n",
    "\n",
    "\n",
    "for df in [df1, df2,df3,df4,df5,df6]:\n",
    "    df.columns = df.columns.str.strip().str.lower()  \n",
    "\n",
    "# Merge both datasets\n",
    "merged_df = pd.concat([df1, df2,df3,df4,df5,df6], ignore_index=True)  \n",
    "\n",
    "\n",
    "\n",
    "# Save to a new CSV file\n",
    "merged_df.to_csv(\"C:/Users/Etu/Desktop/api_domaine_info.csv\", index=False ,  encoding = \"utf-8-sig\")  \n",
    "\n",
    "len(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8a74ea19-1ba7-4fd3-b1e2-04002ceed9c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Correction...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re \n",
    "\n",
    "df = pd.read_csv(\"C:/Users/Etu/Desktop/api_domaine_info.csv\",  encoding = \"utf-8\") \n",
    "\n",
    "\n",
    "def format_salaire_intelligent(texte):\n",
    "    if pd.isna(texte) or texte == \"\":\n",
    "        return \"Non sp√©cifi√©\"\n",
    "\n",
    "    texte_lower = str(texte).lower().replace(\" \", \"\")\n",
    "\n",
    "    # --- 1. D√âTECTION DU MULTIPLICATEUR (Correction ici) ---\n",
    "    # R√®gle absolue : Si \"annuel\" est pr√©sent, c'est x1. Point barre.\n",
    "    if \"annuel\" in texte_lower:\n",
    "        multiplicateur = 1\n",
    "    elif \"mensuel\" in texte_lower or \"mois\" in texte_lower:\n",
    "        multiplicateur = 12\n",
    "    else:\n",
    "        multiplicateur = 1\n",
    "\n",
    "    # --- 2. EXTRACTION DES CHIFFRES ---\n",
    "    matches = re.findall(r\"(\\d+(?:\\.\\d+)?)\", texte_lower)\n",
    "    if not matches: return \"Non sp√©cifi√©\"\n",
    "\n",
    "    valeurs_propres = []\n",
    "    \n",
    "    for m in matches:\n",
    "        try:\n",
    "            # On calcule la valeur annuelle potentielle\n",
    "            val = float(m) * multiplicateur\n",
    "            \n",
    "            # --- 3. FILTRE ANTI-BRUIT (Important !) ---\n",
    "            # On ignore les chiffres comme \"12.0\" (mois) ou \"35\" (heures)\n",
    "            # Un salaire annuel r√©aliste est g√©n√©ralement > 14 000 ‚Ç¨ (SMIC)\n",
    "            if val > 14000:\n",
    "                # Formatage propre : 25000.0 -> \"25 000\"\n",
    "                val_str = \"{:,.0f}\".format(val).replace(\",\", \" \")\n",
    "                valeurs_propres.append(val_str)\n",
    "        except: pass\n",
    "\n",
    "    # --- 4. FORMATAGE FINAL ---\n",
    "    # On enl√®ve les doublons et on trie (pour avoir petit - grand)\n",
    "    valeurs_propres = sorted(list(set(valeurs_propres)))\n",
    "\n",
    "    if len(valeurs_propres) >= 2:\n",
    "        return f\"{valeurs_propres[0]} - {valeurs_propres[-1]} ‚Ç¨ / an\"\n",
    "    elif len(valeurs_propres) == 1:\n",
    "        return f\"{valeurs_propres[0]} ‚Ç¨ / an\"\n",
    "\n",
    "    return \"Non sp√©cifi√©\"\n",
    "\n",
    "# Test\n",
    "print(\"üîÑ Correction...\")\n",
    "df['salaire_clean'] = df['salaire'].apply(format_salaire_intelligent)\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "\n",
    "\n",
    "df.to_csv(\"C:/Users/Etu/Desktop/api_domaine_infooo.csv\",  encoding = \"utf-8-sig\",index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "13a21222-6f37-44a2-94fe-246d3f5b52ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Cr√©ation de la colonne R√©gion...\n",
      "                                ville departement               region_finale\n",
      "0                      03 - Montoldre           3        Auvergne-Rh√¥ne-Alpes\n",
      "1                       06 - VALBONNE           6  Provence-Alpes-C√¥te d'Azur\n",
      "2                       06 - Valbonne           6  Provence-Alpes-C√¥te d'Azur\n",
      "3               10 - Nogent-sur-Seine          10                   Grand Est\n",
      "4                  11 - Castelnaudary          11                   Occitanie\n",
      "5                   13 - MARSEILLE 10          13  Provence-Alpes-C√¥te d'Azur\n",
      "6                13 - AIX EN PROVENCE          13  Provence-Alpes-C√¥te d'Azur\n",
      "7    13 - Marseille 8e Arrondissement          13  Provence-Alpes-C√¥te d'Azur\n",
      "8                13 - Aix-en-Provence          13  Provence-Alpes-C√¥te d'Azur\n",
      "9                          13 - Arles          13  Provence-Alpes-C√¥te d'Azur\n",
      "10  13 - Marseille 16e Arrondissement          13  Provence-Alpes-C√¥te d'Azur\n",
      "11               21 - Semur-en-Auxois          21     Bourgogne-Franche-Comt√©\n",
      "12                     21 - C√¥te-d'Or          21     Bourgogne-Franche-Comt√©\n",
      "13                  22 - Saint-Brieuc          22                    Bretagne\n",
      "14                     29 - Plou√©dern          29                    Bretagne\n",
      "‚úÖ Fichier sauvegard√© : C:/Users/Etu/Desktop/dataset_COMPLETE_REGIONS.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Chargement du fichier\n",
    "# Remplace par ton dernier fichier sauvegard√©\n",
    "input_path = \"C:/Users/Etu/Desktop/api_domain_info.csv\"\n",
    "df = pd.read_csv(input_path)\n",
    "\n",
    "# 2. Dictionnaire de Mapping (D√©partement -> R√©gion)\n",
    "# C'est la carte de France officielle\n",
    "DEPT_TO_REGION = {\n",
    "    # Auvergne-Rh√¥ne-Alpes\n",
    "    \"01\": \"Auvergne-Rh√¥ne-Alpes\", \"03\": \"Auvergne-Rh√¥ne-Alpes\", \"07\": \"Auvergne-Rh√¥ne-Alpes\", \n",
    "    \"15\": \"Auvergne-Rh√¥ne-Alpes\", \"26\": \"Auvergne-Rh√¥ne-Alpes\", \"38\": \"Auvergne-Rh√¥ne-Alpes\", \n",
    "    \"42\": \"Auvergne-Rh√¥ne-Alpes\", \"43\": \"Auvergne-Rh√¥ne-Alpes\", \"63\": \"Auvergne-Rh√¥ne-Alpes\", \n",
    "    \"69\": \"Auvergne-Rh√¥ne-Alpes\", \"73\": \"Auvergne-Rh√¥ne-Alpes\", \"74\": \"Auvergne-Rh√¥ne-Alpes\",\n",
    "    \n",
    "    # Bourgogne-Franche-Comt√©\n",
    "    \"21\": \"Bourgogne-Franche-Comt√©\", \"25\": \"Bourgogne-Franche-Comt√©\", \"39\": \"Bourgogne-Franche-Comt√©\", \n",
    "    \"58\": \"Bourgogne-Franche-Comt√©\", \"70\": \"Bourgogne-Franche-Comt√©\", \"71\": \"Bourgogne-Franche-Comt√©\", \n",
    "    \"89\": \"Bourgogne-Franche-Comt√©\", \"90\": \"Bourgogne-Franche-Comt√©\",\n",
    "    \n",
    "    # Bretagne\n",
    "    \"22\": \"Bretagne\", \"29\": \"Bretagne\", \"35\": \"Bretagne\", \"56\": \"Bretagne\",\n",
    "    \n",
    "    # Centre-Val de Loire\n",
    "    \"18\": \"Centre-Val de Loire\", \"28\": \"Centre-Val de Loire\", \"36\": \"Centre-Val de Loire\", \n",
    "    \"37\": \"Centre-Val de Loire\", \"41\": \"Centre-Val de Loire\", \"45\": \"Centre-Val de Loire\",\n",
    "    \n",
    "    # Corse\n",
    "    \"2A\": \"Corse\", \"2B\": \"Corse\",\n",
    "    \n",
    "    # Grand Est\n",
    "    \"08\": \"Grand Est\", \"10\": \"Grand Est\", \"51\": \"Grand Est\", \"52\": \"Grand Est\", \"54\": \"Grand Est\", \n",
    "    \"55\": \"Grand Est\", \"57\": \"Grand Est\", \"67\": \"Grand Est\", \"68\": \"Grand Est\", \"88\": \"Grand Est\",\n",
    "    \n",
    "    # Hauts-de-France\n",
    "    \"02\": \"Hauts-de-France\", \"59\": \"Hauts-de-France\", \"60\": \"Hauts-de-France\", \"62\": \"Hauts-de-France\", \"80\": \"Hauts-de-France\",\n",
    "    \n",
    "    # √éle-de-France\n",
    "    \"75\": \"√éle-de-France\", \"77\": \"√éle-de-France\", \"78\": \"√éle-de-France\", \"91\": \"√éle-de-France\", \n",
    "    \"92\": \"√éle-de-France\", \"93\": \"√éle-de-France\", \"94\": \"√éle-de-France\", \"95\": \"√éle-de-France\",\n",
    "    \n",
    "    # Normandie\n",
    "    \"14\": \"Normandie\", \"27\": \"Normandie\", \"50\": \"Normandie\", \"61\": \"Normandie\", \"76\": \"Normandie\",\n",
    "    \n",
    "    # Nouvelle-Aquitaine\n",
    "    \"16\": \"Nouvelle-Aquitaine\", \"17\": \"Nouvelle-Aquitaine\", \"19\": \"Nouvelle-Aquitaine\", \"23\": \"Nouvelle-Aquitaine\", \n",
    "    \"24\": \"Nouvelle-Aquitaine\", \"33\": \"Nouvelle-Aquitaine\", \"40\": \"Nouvelle-Aquitaine\", \"47\": \"Nouvelle-Aquitaine\", \n",
    "    \"64\": \"Nouvelle-Aquitaine\", \"79\": \"Nouvelle-Aquitaine\", \"86\": \"Nouvelle-Aquitaine\", \"87\": \"Nouvelle-Aquitaine\",\n",
    "    \n",
    "    # Occitanie\n",
    "    \"09\": \"Occitanie\", \"11\": \"Occitanie\", \"12\": \"Occitanie\", \"30\": \"Occitanie\", \"31\": \"Occitanie\", \n",
    "    \"32\": \"Occitanie\", \"34\": \"Occitanie\", \"46\": \"Occitanie\", \"48\": \"Occitanie\", \"65\": \"Occitanie\", \n",
    "    \"66\": \"Occitanie\", \"81\": \"Occitanie\", \"82\": \"Occitanie\",\n",
    "    \n",
    "    # Pays de la Loire\n",
    "    \"44\": \"Pays de la Loire\", \"49\": \"Pays de la Loire\", \"53\": \"Pays de la Loire\", \"72\": \"Pays de la Loire\", \"85\": \"Pays de la Loire\",\n",
    "    \n",
    "    # Provence-Alpes-C√¥te d'Azur\n",
    "    \"04\": \"Provence-Alpes-C√¥te d'Azur\", \"05\": \"Provence-Alpes-C√¥te d'Azur\", \"06\": \"Provence-Alpes-C√¥te d'Azur\", \n",
    "    \"13\": \"Provence-Alpes-C√¥te d'Azur\", \"83\": \"Provence-Alpes-C√¥te d'Azur\", \"84\": \"Provence-Alpes-C√¥te d'Azur\",\n",
    "    \n",
    "    # DOM-TOM (Principaux)\n",
    "    \"971\": \"Guadeloupe\", \"972\": \"Martinique\", \"973\": \"Guyane\", \"974\": \"La R√©union\", \"976\": \"Mayotte\"\n",
    "}\n",
    "\n",
    "# 3. Fonction pour trouver la r√©gion \n",
    "def get_region_from_dept(dept_code):\n",
    "    if pd.isna(dept_code) or dept_code == \"\" or dept_code == \"Non sp√©cifi√©\":\n",
    "        return \"Non sp√©cifi√©\"\n",
    "    \n",
    "    # Normalisation : on transforme tout en cha√Æne de caract√®res\n",
    "    code = str(dept_code).strip().upper()\n",
    "    \n",
    "    # Gestion des chiffres simples (ex: 6 devient \"06\")\n",
    "    if code.isdigit() and len(code) == 1:\n",
    "        code = \"0\" + code \n",
    "        \n",
    "    # On regarde dans le dictionnaire\n",
    "    return DEPT_TO_REGION.get(code, \"Autre / International\")\n",
    "\n",
    "print(\"üîÑ Cr√©ation de la colonne R√©gion...\")\n",
    "\n",
    "# 4. Application sur la colonne 'departement'\n",
    "# Assure-toi que ta colonne s'appelle bien 'departement'\n",
    "if 'departement' in df.columns:\n",
    "    df['region_finale'] = df['departement'].apply(get_region_from_dept)\n",
    "    \n",
    "    # V√©rification\n",
    "    print(df[['ville', 'departement', 'region_finale']].head(15))\n",
    "    \n",
    "    # Sauvegarde\n",
    "    output_path = \"C:/Users/Etu/Desktop/dataset_COMPLETE_REGIONS.csv\"\n",
    "    df.to_csv(output_path, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"‚úÖ Fichier sauvegard√© : {output_path}\")\n",
    "else:\n",
    "    print(\"‚ùå Erreur : La colonne 'departement' n'existe pas dans le fichier.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0322b9b9-401d-4f41-958f-90ef0072de45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Normalisation de l'exp√©rience...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "input_path = \"C:/Users/Etu/Desktop/cleaned_informatique.csv\"\n",
    "df = pd.read_csv(input_path)\n",
    "\n",
    "# 2. Fonction de nettoyage\n",
    "def classer_experience(texte):\n",
    "    if pd.isna(texte) or texte == \"\":\n",
    "        return \"Non sp√©cifi√©\"\n",
    "    \n",
    "    texte_clean = str(texte).lower().strip()\n",
    "\n",
    "    # --- CAS 1 : Mots-cl√©s explicites ---\n",
    "    if \"d√©butant\" in texte_clean:\n",
    "        return \"Junior (0-2 ans)\"\n",
    "\n",
    "    # --- CAS 2 : D√©tection des MOIS (ex: \"24 Mois\") ---\n",
    "    # On cherche un chiffre avant le mot \"mois\"\n",
    "    match_mois = re.search(r\"(\\d+)\\s*mois\", texte_clean)\n",
    "    if match_mois:\n",
    "        mois = int(match_mois.group(1))\n",
    "        annees = mois / 12\n",
    "        \n",
    "        if annees <= 2: return \"Junior (0-2 ans)\"\n",
    "        elif 2 < annees <= 5: return \"Interm√©diaire (2-5 ans)\"\n",
    "        else: return \"Senior (5+ ans)\"\n",
    "\n",
    "    # --- CAS 3 : D√©tection des ANN√âES (ex: \"5 An(s)\") ---\n",
    "    # On cherche un chiffre avant \"an\" (marche pour An, Ans, An(s))\n",
    "    match_an = re.search(r\"(\\d+)\\s*an\", texte_clean)\n",
    "    if match_an:\n",
    "        annees = int(match_an.group(1))\n",
    "        \n",
    "        if annees <= 2: return \"Junior (0-2 ans)\"\n",
    "        elif 2 < annees <= 5: return \"Interm√©diaire (2-5 ans)\"\n",
    "        else: return \"Senior (5+ ans)\"\n",
    "\n",
    "    # --- CAS 4 : \"Exp√©rience exig√©e\" sans chiffre ---\n",
    "    # Souvent, si on exige de l'exp√©rience sans pr√©ciser, c'est au moins interm√©diaire\n",
    "    if \"exp√©rience\" in texte_clean or \"exig√©e\" in texte_clean:\n",
    "        return \"Interm√©diaire (2-5 ans)\" # Choix par d√©faut s√©curis√©\n",
    "\n",
    "    return \"Non sp√©cifi√©\"\n",
    "\n",
    "# 3. Application\n",
    "print(\"üîÑ Normalisation de l'exp√©rience...\")\n",
    "df['experience_normalisee'] = df['experience'].apply(classer_experience)\n",
    "\n",
    "# 4. R√©sultat\n",
    "df.to_csv(\"C:/Users/Etu/Desktop/domaine.csv\", encoding = \"utf-8-sig\" ,index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "dbd1d642-4ee4-4a84-9e7f-38e645191888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Cr√©ation de la colonne 'metier'...\n",
      "\n",
      "üìä Aper√ßu des r√©sultats (Top 20 cat√©gories) :\n",
      "metier\n",
      "Data Engineer                                154\n",
      "Data Analyst                                 144\n",
      "D√©veloppeur Web / Fullstack                  142\n",
      "Intelligence Artificielle                    136\n",
      "D√©veloppeur Logiciel                          82\n",
      "Data Scientist                                59\n",
      "D√©veloppeur                                   35\n",
      "Directeur / CTO                               27\n",
      "Chef de Projet / PO                           11\n",
      "DevOps / Cloud                                11\n",
      "Consultant                                     9\n",
      "Ing√©nieur D√©veloppement Logiciel Embarqu√©      8\n",
      "Ing√©nieur D√©veloppement Logiciel               7\n",
      "Syst√®me & R√©seau                               7\n",
      "D√©veloppeur Mobile                             5\n",
      "Test / QA                                      4\n",
      "Data Ing√©nieur                                 3\n",
      "Ing√©nieur Donn√©es Industrielles                3\n",
      "Contr√¥leur De Gestion                          3\n",
      "Cybers√©curit√©                                  3\n",
      "Name: count, dtype: int64\n",
      "\n",
      "‚ÑπÔ∏è Nombre de titres conserv√©s tels quels : 211\n",
      "Exemple : Master Data Officer - Lille\n",
      "‚úÖ Fichier sauvegard√© : C:/Users/Etu/Desktop/dataset_FINAL_METIERS_V2.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Charger ton fichier\n",
    "input_path = \"C:/Users/Etu/Desktop/informatique_FINAL_SKILLS.csv\" # Ton fichier actuel\n",
    "df = pd.read_csv(input_path)\n",
    "\n",
    "# 2. D√©finition des Cat√©gories\n",
    "# L'ordre est important : le script prend la premi√®re correspondance trouv√©e.\n",
    "CATEGORIES_METIER = {\n",
    "    # --- NOUVELLE CAT√âGORIE IA (Prioritaire) ---\n",
    "    \"Intelligence Artificielle\": [\n",
    "        \"intelligence artificielle\", \"artificial intelligence\", \" ia \", \" ai \", \"-ia\", \"-ai\",\n",
    "        \"machine learning\", \"deep learning\", \"computer vision\", \"nlp\", \"llm\", \n",
    "        \"g√©n√©rative\", \"generative\", \"rag \", \"chatbot\", \"vision par ordinateur\",\n",
    "        \"ing√©nieur ia\", \"ai engineer\"\n",
    "    ],\n",
    "\n",
    "    # --- DATA ---\n",
    "    \"Data Scientist\": [\"data scientist\", \"data science\", \"scientist\"],\n",
    "    \"Data Analyst\": [\"data analyst\", \"business analyst\", \"bi \", \"business intelligence\", \"analytics\", \"analyste de donn√©es\"],\n",
    "    \"Data Engineer\": [\"data engineer\", \"ing√©nieur data\", \"big data\", \"etl\", \"spark\", \"hadoop\", \"databricks\"],\n",
    "    \"Data Architect\": [\"data architect\", \"architecte de donn√©es\", \"data manager\", \"data steward\"],\n",
    "    \n",
    "    # --- D√âVELOPPEMENT ---\n",
    "    \"D√©veloppeur Web / Fullstack\": [\"fullstack\", \"full-stack\", \"web\", \"react\", \"angular\", \"vue\", \"node\", \"front-end\", \"back-end\", \"javascript\", \"php\", \"symfony\", \"laravel\", \"wordpress\"],\n",
    "    \"D√©veloppeur Mobile\": [\"mobile\", \"android\", \"ios\", \"flutter\", \"react native\", \"swift\", \"kotlin\"],\n",
    "    \"D√©veloppeur Logiciel\": [\"software engineer\", \"ing√©nieur logiciel\", \"d√©veloppeur logiciel\", \"c++\", \"c#\", \".net\", \"java \", \"j2ee\", \"python\", \"golang\", \"rust\", \"ruby\"],\n",
    "    \"D√©veloppeur\": [\"d√©veloppeur\", \"developer\", \"dev \", \"programmeur\", \"codeur\"],\n",
    "    \n",
    "    # --- INFRA & CLOUD ---\n",
    "    \"DevOps / Cloud\": [\"devops\", \"cloud\", \"aws\", \"azure\", \"gcp\", \"sre\", \"ci/cd\", \"kubernetes\", \"docker\", \"terraform\"],\n",
    "    \"Cybers√©curit√©\": [\"cybers√©curit√©\", \"cyber\", \"s√©curit√©\", \"security\", \"pentester\", \"soc \", \"siem\", \"rssi\"],\n",
    "    \"Syst√®me & R√©seau\": [\"syst√®me\", \"r√©seau\", \"administrateur\", \"admin sys\", \"ing√©nieur r√©seau\", \"technicien support\", \"helpdesk\", \"support it\"],\n",
    "    \n",
    "    # --- MANAGEMENT ---\n",
    "    \"Chef de Projet / PO\": [\"chef de projet\", \"project manager\", \"product owner\", \"product manager\", \"scrum master\", \"cp \", \"po \"],\n",
    "    \"Directeur / CTO\": [\"cto\", \"directeur technique\", \"head of\", \"lead\", \"responsable\", \"manager\", \"directeur\"],\n",
    "    \n",
    "    # --- AUTRES IT ---\n",
    "    \"Consultant\": [\"consultant\", \"audit\"],\n",
    "    \"Test / QA\": [\"testeur\", \"qa \", \"qualit√©\", \"validation\"]\n",
    "}\n",
    "\n",
    "def categoriser_metier_smart(titre):\n",
    "    # Si le titre est vide, on renvoie \"Non sp√©cifi√©\"\n",
    "    if pd.isna(titre) or titre == \"\":\n",
    "        return \"Non sp√©cifi√©\"\n",
    "    \n",
    "    # Nettoyage pour la recherche (minuscule)\n",
    "    titre_clean = str(titre).lower()\n",
    "    \n",
    "    # 1. On cherche dans le dictionnaire\n",
    "    for metier_cat, mots_cles in CATEGORIES_METIER.items():\n",
    "        for mot in mots_cles:\n",
    "            if mot in titre_clean:\n",
    "                return metier_cat # Correspondance trouv√©e !\n",
    "            \n",
    "    # 2. SI AUCUNE CORRESPONDANCE : On renvoie le titre original\n",
    "    # On le met juste en format \"Titre\" (Majuscule au d√©but) pour que ce soit propre\n",
    "    return str(titre).strip().title()\n",
    "\n",
    "print(\"üîÑ Cr√©ation de la colonne 'metier'...\")\n",
    "\n",
    "# 3. Application de la fonction\n",
    "df['metier'] = df['titre'].apply(categoriser_metier_smart)\n",
    "\n",
    "# 4. V√©rification\n",
    "print(\"\\nüìä Aper√ßu des r√©sultats (Top 20 cat√©gories) :\")\n",
    "print(df['metier'].value_counts().head(20))\n",
    "\n",
    "# Petit check pour voir les titres originaux conserv√©s (ceux qui ne sont pas dans les cl√©s du dico)\n",
    "categories_standard = list(CATEGORIES_METIER.keys())\n",
    "titres_non_standard = df[~df['metier'].isin(categories_standard)]\n",
    "print(f\"\\n‚ÑπÔ∏è Nombre de titres conserv√©s tels quels : {len(titres_non_standard)}\")\n",
    "if not titres_non_standard.empty:\n",
    "    print(\"Exemple :\", titres_non_standard['metier'].iloc[0])\n",
    "\n",
    "# 5. Sauvegarde\n",
    "output_path = \"C:/Users/Etu/Desktop/dataset_FINAL_METIERS_V2.csv\"\n",
    "df.to_csv(output_path, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"‚úÖ Fichier sauvegard√© : {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4bd4aac3-7691-47ce-b256-1747a4c33906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Correction des salaires minimums aberrants (< 10k)...\n",
      "‚ö†Ô∏è 29 lignes d√©tect√©es avec un salaire min < 10 000 ‚Ç¨.\n",
      "üîÑ Recalcul de la moyenne (Avg)...\n",
      "‚úÖ Fichier corrig√© sauvegard√© : C:/Users/Etu/Desktop/dataset_FINAL_CLEANED_SALARY.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Charger ton fichier (mets le bon chemin)\n",
    "input_path = \"C:/Users/Etu/Desktop/cleaned_domain_info1.csv\"\n",
    "df = pd.read_csv(input_path)\n",
    "\n",
    "# V√©rifions les noms exacts de tes colonnes (adapte si besoin)\n",
    "# Supposons qu'elles s'appellent 'salaire_min' et 'salaire_max'\n",
    "col_min = 'salary_min' \n",
    "col_max = 'salary_max'\n",
    "col_avg = 'avg_salary' # On devra la recalculer apr√®s !\n",
    "\n",
    "print(\"üîÑ Correction des salaires minimums aberrants (< 10k)...\")\n",
    "\n",
    "# 2. LA CONDITION (Le Masque)\n",
    "# On cherche toutes les lignes o√π le salaire_min est inf√©rieur √† 10 000 ET non vide\n",
    "condition = (df[col_min] < 10000) & (df[col_min].notna())\n",
    "\n",
    "# Affichage du nombre de lignes concern√©es avant modif\n",
    "nb_erreurs = condition.sum()\n",
    "print(f\"‚ö†Ô∏è {nb_erreurs} lignes d√©tect√©es avec un salaire min < 10 000 ‚Ç¨.\")\n",
    "\n",
    "# 3. LA CORRECTION\n",
    "# Pour ces lignes-l√†, on remplace Min par Max * 0.85 (soit Max - 15%)\n",
    "df.loc[condition, col_min] = df.loc[condition, col_max] * 0.85\n",
    "\n",
    "# 4. MISE √Ä JOUR DE LA MOYENNE (Important !)\n",
    "# Si on change le Min, la moyenne (Min+Max)/2 change aussi. Il faut la recalculer.\n",
    "print(\"üîÑ Recalcul de la moyenne (Avg)...\")\n",
    "df[col_avg] = (df[col_min] + df[col_max]) / 2\n",
    "\n",
    "# 5. Sauvegarde\n",
    "output_path = \"C:/Users/Etu/Desktop/dataset_FINAL_CLEANED_SALARY.csv\"\n",
    "df.to_csv(output_path, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"‚úÖ Fichier corrig√© sauvegard√© : {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6dbc886d-eadc-4882-a4e5-9dafab70c0c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fichier 1 : 6166 lignes\n",
      "Fichier 2 : 1040 lignes\n",
      "‚úÖ Total fusionn√© : 7206 lignes\n",
      "üéâ Fichier final cr√©√© et tri√© !\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7206"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Charger les 3 fichiers\n",
    "# Remplace par tes vrais chemins\n",
    "path = \"C:/Users/Etu/Desktop/\"\n",
    "df1 = pd.read_csv(path + \"data_set_final.csv\")\n",
    "df2 = pd.read_csv(path + \"cleaned_info2.csv\")\n",
    "\n",
    "\n",
    "print(f\"Fichier 1 : {len(df1)} lignes\")\n",
    "print(f\"Fichier 2 : {len(df2)} lignes\")\n",
    "\n",
    "\n",
    "\n",
    "# 3. La Fusion Magique\n",
    "# ignore_index=True permet de refaire la num√©rotation des lignes de 0 √† la fin\n",
    "df_final = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "print(f\"‚úÖ Total fusionn√© : {len(df_final)} lignes\")\n",
    "\n",
    "\n",
    "ordre_desire = [ \n",
    "    \"id\",\n",
    "    \"titre\",\n",
    "    \"metier\",\n",
    "    \"entreprise\",\n",
    "    \"salaire_proposer\",\n",
    "    \"salaire_min\",\n",
    "    \"salaire_max\",\n",
    "    \"salaire_avg\",\n",
    "    \"description\",\n",
    "    \"experience\",\n",
    "    \"competences\",\n",
    "    \"ville\",\n",
    "    \"departement\",\n",
    "    \"region\"\n",
    "    \n",
    "    \n",
    "]\n",
    "\n",
    "# On filtre pour ne garder que les colonnes qui existent vraiment\n",
    "colonnes_presentes = [c for c in ordre_desire if c in df_final.columns]\n",
    "\n",
    "# On applique l'ordre\n",
    "df_final = df_final[colonnes_presentes]\n",
    "\n",
    "# 5. Sauvegarde\n",
    "df_final.to_csv(path + \"dataset_COMPLET_MERGED.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "print(\" Fichier final cr√©√© et tri√© !\")\n",
    "len(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4c643d05-ad25-41f5-a9ae-29594f094767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Correction insensible √† la casse (Case Insensitive)...\n",
      "Exemple de correction :\n",
      "                                            metier  \\\n",
      "297                                 Data Scientist   \n",
      "936                      Intelligence Artificielle   \n",
      "384                                Directeur / CTO   \n",
      "950                            Chef de Projet / PO   \n",
      "1001  Conducteur Vl - Cdd - Strasbourg - Permis Eb   \n",
      "\n",
      "                                                    competences_corrigees  \n",
      "297     python, sql, machine learning, scikit-learn, pandas, statistiques  \n",
      "936                                    angular, java, spring, spring boot  \n",
      "384        management, strat√©gie, budget, leadership, architecture, agile  \n",
      "950   agile, angular, cobol, confluence, java, jira, mysql, python, scrum  \n",
      "1001                                                     java,python, git  \n",
      "‚úÖ Fichier corrig√© sauvegard√© : C:/Users/Etu/Desktop/dataset_FINAL_SKILLS_FIXED.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Charger ton fichier\n",
    "input_path = \"C:/Users/Etu/Desktop/dataset_FINAL_METIERS_V2.csv\"\n",
    "df = pd.read_csv(input_path)\n",
    "\n",
    "# 2. Dictionnaire Humain (Lisible)\n",
    "DEFAULT_SKILLS_RAW = {\n",
    "    \"Data Scientist\": \"python, sql, machine learning, scikit-learn, pandas, statistiques\",\n",
    "    \"Data Analyst\": \"sql, excel, power bi, tableau, python, analyse de donn√©es\",\n",
    "    \"Data Engineer\": \"python, sql, spark, etl, big data, cloud\",\n",
    "    \"Data Architect\": \"sql, nosql, cloud, data modeling, architecture, s√©curit√©\",\n",
    "    \"Intelligence Artificielle\": \"python, tensorflow, pytorch, deep learning, nlp, computer vision\",\n",
    "    \"D√©veloppeur Web / Fullstack\": \"javascript, html, css, react, node.js, git\",\n",
    "    \"D√©veloppeur Mobile\": \"swift, kotlin, flutter, react native, ios, android\",\n",
    "    \"D√©veloppeur Logiciel\": \"java, c++, c#, python, algorithmes, git\",\n",
    "    \"D√©veloppeur\": \"git, programmation, algorithmes, base de donn√©es, code review\",\n",
    "    \"DevOps / Cloud\": \"docker, kubernetes, aws, ci/cd, linux, terraform\",\n",
    "    \"Cybers√©curit√©\": \"r√©seau, s√©curit√©, linux, python, firewall, audit\",\n",
    "    \"Syst√®me & R√©seau\": \"linux, windows server, r√©seau, tcp/ip, vmware, active directory\",\n",
    "    \"Chef de Projet / PO\": \"gestion de projet, agile, scrum, jira, communication, planning\",\n",
    "    \"Directeur / CTO\": \"management, strat√©gie, budget, leadership, architecture, agile\",\n",
    "    \"Consultant\": \"communication, analyse, gestion de projet, excel, powerpoint\",\n",
    "    \"Test / QA\": \"test manuel, test automatis√©, selenium, jira, qualit√©, bug tracking\"\n",
    "}\n",
    "\n",
    "# --- CORRECTION ICI : ON CR√âE UN DICTIONNAIRE EN MINUSCULE ---\n",
    "# On transforme toutes les cl√©s en minuscules pour la comparaison\n",
    "SKILLS_MAPPING = {k.lower(): v for k, v in DEFAULT_SKILLS_RAW.items()}\n",
    "\n",
    "FALLBACK_SKILLS = \"java,python, git\"\n",
    "\n",
    "def enrichir_competences(row):\n",
    "    # R√©cup√©ration et nettoyage du m√©tier (TOUT EN MINUSCULE)\n",
    "    metier = str(row.get('metier', '')).strip().lower()\n",
    "    \n",
    "    competences = str(row.get('competences', ''))\n",
    "    competences_lower = competences.lower()\n",
    "    \n",
    "    # 1. ANALYSE : Est-ce qu'on doit remplacer ?\n",
    "    # Cas vide ou \"non sp√©cifi√©\"\n",
    "    is_empty = (pd.isna(competences) or \n",
    "                competences == \"\" or \n",
    "                \"non sp√©cifi√©\" in competences_lower or \n",
    "                \"non renseigner\" in competences_lower or\n",
    "                \"nan\" == competences_lower)\n",
    "    \n",
    "    # Cas liste trop courte (< 3 comp√©tences)\n",
    "    nb_skills = len(competences.split(',')) if not is_empty else 0\n",
    "    is_sparse = nb_skills < 3\n",
    "    \n",
    "    # 2. REMPLACEMENT\n",
    "    if is_empty or is_sparse:\n",
    "        # On cherche dans le dictionnaire MINUSCULE\n",
    "        # Cela marchera que le m√©tier soit \"Data Scientist\" ou \"DATA SCIENTIST\"\n",
    "        new_skills = SKILLS_MAPPING.get(metier, FALLBACK_SKILLS)\n",
    "        return new_skills\n",
    "        \n",
    "    return competences\n",
    "\n",
    "print(\"üîÑ Correction insensible √† la casse (Case Insensitive)...\")\n",
    "\n",
    "# Application\n",
    "df['competences_corrigees'] = df.apply(enrichir_competences, axis=1)\n",
    "\n",
    "# V√©rification sur un m√©tier √©crit en majuscule/minuscule pour voir si √ßa matche\n",
    "print(\"Exemple de correction :\")\n",
    "print(df[['metier', 'competences_corrigees']].sample(5))\n",
    "\n",
    "# Finalisation\n",
    "df['competences'] = df['competences_corrigees']\n",
    "df.drop(columns=['competences_corrigees'], inplace=True)\n",
    "\n",
    "# Sauvegarde\n",
    "output_path = \"C:/Users/Etu/Desktop/dataset_FINAL_SKILLS_FIXED.csv\"\n",
    "df.to_csv(output_path, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"‚úÖ Fichier corrig√© sauvegard√© : {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6315ec7d-a708-4d18-a6dc-40139dd51d1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
